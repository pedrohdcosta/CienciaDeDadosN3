{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Notebook 02 - Modelagem e Avalia\u00e7\u00e3o Comparativa\n",
        "\n",
        "**Projeto:** Previs\u00e3o de Churn em Telecomunica\u00e7\u00f5es  \n",
        "**Autores:** Pedro Dias, Gustavo Rodrigues  \n",
        "**Data:** Dezembro 2025\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Treinar e comparar **5 modelos de classifica\u00e7\u00e3o** diferentes:\n",
        "1. Decision Tree (\u00c1rvore de Decis\u00e3o)\n",
        "2. Random Forest (Floresta Aleat\u00f3ria)\n",
        "3. Logistic Regression (Regress\u00e3o Log\u00edstica)\n",
        "4. K-Nearest Neighbors (KNN)\n",
        "5. Support Vector Machine (SVM)\n",
        "\n",
        "Avaliar cada modelo usando **4 m\u00e9tricas:**\n",
        "- Acur\u00e1cia\n",
        "- Precis\u00e3o\n",
        "- Recall\n",
        "- F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== SETUP ==========\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Modelos\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# M\u00e9tricas\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        ")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"\u2705 Bibliotecas carregadas com sucesso!\")\n",
        "print(f\"\\nVers\u00f5es:\")\n",
        "import sklearn\n",
        "print(f\"  Scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"  Pandas: {pd.__version__}\")\n",
        "print(f\"  NumPy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregamento e Prepara\u00e7\u00e3o dos Dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dataset\n",
        "print(\"\ud83d\udcca Carregando dataset...\\n\")\n",
        "url = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url)\n",
        "    print(\"\u2705 Dataset carregado!\")\n",
        "except:\n",
        "    url_alt = \"https://raw.githubusercontent.com/marvin-rubia/Churn-Analysis-Prediction/main/WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
        "    df = pd.read_csv(url_alt)\n",
        "    print(\"\u2705 Dataset carregado (URL alternativa)!\")\n",
        "\n",
        "print(f\"Dimens\u00f5es originais: {df.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Limpeza (mesmo processo da EDA)\n",
        "print(\"\\n\ud83d\udd27 Aplicando limpeza dos dados...\\n\")\n",
        "\n",
        "# 1. Converter TotalCharges para num\u00e9rico\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# 2. Remover NAs\n",
        "df_clean = df.dropna(subset=['TotalCharges']).copy()\n",
        "\n",
        "print(f\"\u2705 Limpeza conclu\u00edda!\")\n",
        "print(f\"   Registros removidos: {len(df) - len(df_clean)}\")\n",
        "print(f\"   Dataset final: {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sele\u00e7\u00e3o de features (baseado na EDA)\n",
        "print(\"\\n\ud83c\udfaf Selecionando features...\\n\")\n",
        "\n",
        "selected_features = [\n",
        "    'tenure', 'MonthlyCharges', 'TotalCharges',\n",
        "    'Contract', 'InternetService', 'PaymentMethod',\n",
        "    'OnlineSecurity', 'TechSupport', 'PaperlessBilling',\n",
        "    'SeniorCitizen'\n",
        "]\n",
        "\n",
        "X = df_clean[selected_features].copy()\n",
        "y = df_clean['Churn'].copy()\n",
        "\n",
        "print(f\"Features selecionadas: {len(selected_features)}\")\n",
        "print(f\"Formato X: {X.shape}\")\n",
        "print(f\"Formato y: {y.shape}\")\n",
        "print(f\"\\nDistribui\u00e7\u00e3o do target:\")\n",
        "print(y.value_counts())\n",
        "print(f\"\\nPropor\u00e7\u00e3o: {(y == 'No').sum() / len(y) * 100:.1f}% N\u00e3o-Churn, {(y == 'Yes').sum() / len(y) * 100:.1f}% Churn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Encoding de Vari\u00e1veis Categ\u00f3ricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\ud83d\udd04 Aplicando One-Hot Encoding...\\n\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "X_encoded = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "print(f\"\u2705 Encoding conclu\u00eddo!\")\n",
        "print(f\"   Features antes: {X.shape[1]}\")\n",
        "print(f\"   Features depois: {X_encoded.shape[1]}\")\n",
        "print(f\"\\nNovas colunas criadas:\")\n",
        "print(X_encoded.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Divis\u00e3o Treino/Teste com Estratifica\u00e7\u00e3o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\u2702\ufe0f  Dividindo dados em treino (70%) e teste (30%)...\\n\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded, y,\n",
        "    test_size=0.30,\n",
        "    random_state=42,\n",
        "    stratify=y  # Importante para dataset desbalanceado!\n",
        ")\n",
        "\n",
        "print(f\"\u2705 Divis\u00e3o conclu\u00edda!\")\n",
        "print(f\"\\n\ud83d\udcca Tamanhos:\")\n",
        "print(f\"   Treino: {X_train.shape[0]} amostras\")\n",
        "print(f\"   Teste:  {X_test.shape[0]} amostras\")\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Propor\u00e7\u00f5es em y_train:\")\n",
        "print(y_train.value_counts(normalize=True).round(3))\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Propor\u00e7\u00f5es em y_test:\")\n",
        "print(y_test.value_counts(normalize=True).round(3))\n",
        "\n",
        "print(\"\\n\u2705 Estratifica\u00e7\u00e3o bem-sucedida (propor\u00e7\u00f5es mantidas)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Normaliza\u00e7\u00e3o dos Dados (para KNN e SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\ud83d\udccf Normalizando features num\u00e9ricas...\\n\")\n",
        "\n",
        "# StandardScaler (padroniza\u00e7\u00e3o: m\u00e9dia 0, desvio padr\u00e3o 1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Converter de volta para DataFrame (para manter nomes das colunas)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"\u2705 Normaliza\u00e7\u00e3o conclu\u00edda!\")\n",
        "print(f\"\\nExemplo - Estat\u00edsticas antes da normaliza\u00e7\u00e3o:\")\n",
        "print(X_train[['tenure', 'MonthlyCharges', 'TotalCharges']].describe().loc[['mean', 'std']].round(2))\n",
        "\n",
        "print(f\"\\nExemplo - Estat\u00edsticas depois da normaliza\u00e7\u00e3o:\")\n",
        "print(X_train_scaled[['tenure', 'MonthlyCharges', 'TotalCharges']].describe().loc[['mean', 'std']].round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Defini\u00e7\u00e3o das M\u00e9tricas de Avalia\u00e7\u00e3o\n",
        "\n",
        "### \ud83d\udcca M\u00e9tricas Explicadas\n",
        "\n",
        "#### 1. Acur\u00e1cia (Accuracy)\n",
        "**F\u00f3rmula:** `(VP + VN) / Total`  \n",
        "**O que mede:** Propor\u00e7\u00e3o de predi\u00e7\u00f5es corretas sobre o total.  \n",
        "**Quando usar:** Vis\u00e3o geral do desempenho, mas cuidado com datasets desbalanceados!\n",
        "\n",
        "#### 2. Precis\u00e3o (Precision)\n",
        "**F\u00f3rmula:** `VP / (VP + FP)`  \n",
        "**O que mede:** Das predi\u00e7\u00f5es de churn, quantas estavam corretas.  \n",
        "**Quando usar:** Quando o custo de falsos positivos \u00e9 alto (ex: campanhas caras de reten\u00e7\u00e3o).\n",
        "\n",
        "#### 3. Recall (Sensibilidade)\n",
        "**F\u00f3rmula:** `VP / (VP + FN)`  \n",
        "**O que mede:** Dos clientes que realmente deram churn, quantos identificamos.  \n",
        "**Quando usar:** **CR\u00cdTICO** para churn! \u00c9 melhor \"errar para mais\" do que perder clientes.\n",
        "\n",
        "#### 4. F1-Score\n",
        "**F\u00f3rmula:** `2 \u00d7 (Precis\u00e3o \u00d7 Recall) / (Precis\u00e3o + Recall)`  \n",
        "**O que mede:** M\u00e9dia harm\u00f4nica entre Precis\u00e3o e Recall.  \n",
        "**Quando usar:** Equilibrar precis\u00e3o e recall, \u00fatil em datasets desbalanceados.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf Para o nosso problema de CHURN:\n",
        "- **Recall** \u00e9 a m\u00e9trica mais importante (n\u00e3o podemos perder clientes)\n",
        "- **F1-Score** ajuda a equilibrar com precis\u00e3o\n",
        "- **Acur\u00e1cia** pode ser enganosa devido ao desbalanceamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fun\u00e7\u00e3o para avaliar modelos\n",
        "def avaliar_modelo(nome, modelo, X_train, X_test, y_train, y_test, tempo):\n",
        "    \"\"\"\n",
        "    Avalia um modelo de classifica\u00e7\u00e3o e retorna as m\u00e9tricas.\n",
        "    \"\"\"\n",
        "    # Fazer predi\u00e7\u00f5es\n",
        "    y_pred = modelo.predict(X_test)\n",
        "    \n",
        "    # Calcular m\u00e9tricas\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, pos_label='Yes')\n",
        "    rec = recall_score(y_test, y_pred, pos_label='Yes')\n",
        "    f1 = f1_score(y_test, y_pred, pos_label='Yes')\n",
        "    \n",
        "    # Matriz de confus\u00e3o\n",
        "    cm = confusion_matrix(y_test, y_pred, labels=['No', 'Yes'])\n",
        "    \n",
        "    return {\n",
        "        'Modelo': nome,\n",
        "        'Acur\u00e1cia': acc,\n",
        "        'Precis\u00e3o': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-Score': f1,\n",
        "        'Tempo (s)': tempo,\n",
        "        'Matriz_Confus\u00e3o': cm,\n",
        "        'y_pred': y_pred\n",
        "    }\n",
        "\n",
        "print(\"\u2705 Fun\u00e7\u00e3o de avalia\u00e7\u00e3o definida!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Treinamento e Avalia\u00e7\u00e3o dos Modelos\n",
        "\n",
        "Vamos treinar 5 modelos diferentes e comparar seus desempenhos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Modelo 1: Decision Tree (\u00c1rvore de Decis\u00e3o)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83c\udf33 MODELO 1: DECISION TREE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Treinar\n",
        "start_time = time.time()\n",
        "dt_model = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_time = time.time() - start_time\n",
        "\n",
        "# Avaliar\n",
        "dt_results = avaliar_modelo('Decision Tree', dt_model, X_train, X_test, y_train, y_test, dt_time)\n",
        "\n",
        "print(f\"\u2705 Treinamento conclu\u00eddo em {dt_time:.3f}s\")\n",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas:\")\n",
        "print(f\"   Acur\u00e1cia:  {dt_results['Acur\u00e1cia']:.2%}\")\n",
        "print(f\"   Precis\u00e3o:  {dt_results['Precis\u00e3o']:.2%}\")\n",
        "print(f\"   Recall:    {dt_results['Recall']:.2%}\")\n",
        "print(f\"   F1-Score:  {dt_results['F1-Score']:.2%}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Matriz de Confus\u00e3o:\")\n",
        "print(dt_results['Matriz_Confus\u00e3o'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Modelo 2: Random Forest (Floresta Aleat\u00f3ria)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)",
        "print(\"\ud83c\udf32 MODELO 2: RANDOM FOREST\")",
        "print(\"=\"*80)",
        "",
        "# Treinar",
        "start_time = time.time()",
        "rf_model = RandomForestClassifier(n_estimators=200, max_depth=15, min_samples_split=5, random_state=42, n_jobs=-1)",
        "rf_model.fit(X_train, y_train)",
        "rf_time = time.time() - start_time",
        "",
        "# Avaliar",
        "rf_results = avaliar_modelo('Random Forest', rf_model, X_train, X_test, y_train, y_test, rf_time)",
        "",
        "print(f\"\u2705 Treinamento conclu\u00eddo em {rf_time:.3f}s\")",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas:\")",
        "print(f\"   Acur\u00e1cia:  {rf_results['Acur\u00e1cia']:.2%}\")",
        "print(f\"   Precis\u00e3o:  {rf_results['Precis\u00e3o']:.2%}\")",
        "print(f\"   Recall:    {rf_results['Recall']:.2%}\")",
        "print(f\"   F1-Score:  {rf_results['F1-Score']:.2%}\")",
        "",
        "print(f\"\\n\ud83d\udccb Matriz de Confus\u00e3o:\")",
        "print(rf_results['Matriz_Confus\u00e3o'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Modelo 3: Logistic Regression (Regress\u00e3o Log\u00edstica)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83d\udcc8 MODELO 3: LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Treinar (usa dados normalizados)\n",
        "start_time = time.time()\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_time = time.time() - start_time\n",
        "\n",
        "# Avaliar\n",
        "lr_results = avaliar_modelo('Logistic Regression', lr_model, X_train_scaled, X_test_scaled, y_train, y_test, lr_time)\n",
        "\n",
        "print(f\"\u2705 Treinamento conclu\u00eddo em {lr_time:.3f}s\")\n",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas:\")\n",
        "print(f\"   Acur\u00e1cia:  {lr_results['Acur\u00e1cia']:.2%}\")\n",
        "print(f\"   Precis\u00e3o:  {lr_results['Precis\u00e3o']:.2%}\")\n",
        "print(f\"   Recall:    {lr_results['Recall']:.2%}\")\n",
        "print(f\"   F1-Score:  {lr_results['F1-Score']:.2%}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Matriz de Confus\u00e3o:\")\n",
        "print(lr_results['Matriz_Confus\u00e3o'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 Modelo 4: K-Nearest Neighbors (KNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83d\udc65 MODELO 4: K-NEAREST NEIGHBORS (KNN)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Treinar (usa dados normalizados)\n",
        "start_time = time.time()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=7)\n",
        "knn_model.fit(X_train_scaled, y_train)\n",
        "knn_time = time.time() - start_time\n",
        "\n",
        "# Avaliar\n",
        "knn_results = avaliar_modelo('KNN', knn_model, X_train_scaled, X_test_scaled, y_train, y_test, knn_time)\n",
        "\n",
        "print(f\"\u2705 Treinamento conclu\u00eddo em {knn_time:.3f}s\")\n",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas:\")\n",
        "print(f\"   Acur\u00e1cia:  {knn_results['Acur\u00e1cia']:.2%}\")\n",
        "print(f\"   Precis\u00e3o:  {knn_results['Precis\u00e3o']:.2%}\")\n",
        "print(f\"   Recall:    {knn_results['Recall']:.2%}\")\n",
        "print(f\"   F1-Score:  {knn_results['F1-Score']:.2%}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Matriz de Confus\u00e3o:\")\n",
        "print(knn_results['Matriz_Confus\u00e3o'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Modelo 5: Support Vector Machine (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83c\udfaf MODELO 5: SUPPORT VECTOR MACHINE (SVM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Treinar (usa dados normalizados)\n",
        "start_time = time.time()\n",
        "svm_model = SVC(kernel='rbf', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "svm_time = time.time() - start_time\n",
        "\n",
        "# Avaliar\n",
        "svm_results = avaliar_modelo('SVM', svm_model, X_train_scaled, X_test_scaled, y_train, y_test, svm_time)\n",
        "\n",
        "print(f\"\u2705 Treinamento conclu\u00eddo em {svm_time:.3f}s\")\n",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas:\")\n",
        "print(f\"   Acur\u00e1cia:  {svm_results['Acur\u00e1cia']:.2%}\")\n",
        "print(f\"   Precis\u00e3o:  {svm_results['Precis\u00e3o']:.2%}\")\n",
        "print(f\"   Recall:    {svm_results['Recall']:.2%}\")\n",
        "print(f\"   F1-Score:  {svm_results['F1-Score']:.2%}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udccb Matriz de Confus\u00e3o:\")\n",
        "print(svm_results['Matriz_Confus\u00e3o'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compara\u00e7\u00e3o de Todos os Modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Consolidar resultados\n",
        "all_results = [dt_results, rf_results, lr_results, knn_results, svm_results]\n",
        "\n",
        "comparison_df = pd.DataFrame([\n",
        "    {\n",
        "        'Modelo': r['Modelo'],\n",
        "        'Acur\u00e1cia': r['Acur\u00e1cia'],\n",
        "        'Precis\u00e3o': r['Precis\u00e3o'],\n",
        "        'Recall': r['Recall'],\n",
        "        'F1-Score': r['F1-Score'],\n",
        "        'Tempo (s)': r['Tempo (s)']\n",
        "    }\n",
        "    for r in all_results\n",
        "])\n",
        "\n",
        "# Formatar como porcentagem\n",
        "comparison_df_display = comparison_df.copy()\n",
        "for col in ['Acur\u00e1cia', 'Precis\u00e3o', 'Recall', 'F1-Score']:\n",
        "    comparison_df_display[col] = comparison_df_display[col].apply(lambda x: f\"{x:.2%}\")\n",
        "comparison_df_display['Tempo (s)'] = comparison_df_display['Tempo (s)'].round(2)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83d\udcca COMPARA\u00c7\u00c3O GERAL DOS MODELOS\")\n",
        "print(\"=\"*80)\n",
        "display(comparison_df_display)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiza\u00e7\u00e3o comparativa\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics = ['Acur\u00e1cia', 'Precis\u00e3o', 'Recall', 'F1-Score']\n",
        "colors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\n",
        "\n",
        "for idx, metric in enumerate(metrics):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    \n",
        "    data = comparison_df.sort_values(metric, ascending=True)\n",
        "    \n",
        "    bars = ax.barh(data['Modelo'], data[metric], color=colors[idx], alpha=0.7)\n",
        "    ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
        "    ax.set_title(f'Compara\u00e7\u00e3o: {metric}', fontsize=14, fontweight='bold')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Adicionar valores\n",
        "    for bar in bars:\n",
        "        width = bar.get_width()\n",
        "        ax.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
        "                f'{width:.2%}', ha='left', va='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparacao_modelos.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcbe Gr\u00e1fico salvo como 'comparacao_modelos.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. An\u00e1lise Detalhada do Melhor Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar melhor modelo (por F1-Score)\n",
        "best_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model_name = comparison_df.loc[best_idx, 'Modelo']\n",
        "best_results = all_results[best_idx]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"\ud83c\udfc6 MELHOR MODELO: {best_model_name}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\n\ud83d\udcca M\u00e9tricas Finais:\")\n",
        "print(f\"   Acur\u00e1cia:  {best_results['Acur\u00e1cia']:.2%}\")\n",
        "print(f\"   Precis\u00e3o:  {best_results['Precis\u00e3o']:.2%}\")\n",
        "print(f\"   Recall:    {best_results['Recall']:.2%}\")\n",
        "print(f\"   F1-Score:  {best_results['F1-Score']:.2%}\")\n",
        "print(f\"   Tempo:     {best_results['Tempo (s)']:.3f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matriz de Confus\u00e3o detalhada\n",
        "cm = best_results['Matriz_Confus\u00e3o']\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['N\u00e3o Churn', 'Churn'],\n",
        "            yticklabels=['N\u00e3o Churn', 'Churn'],\n",
        "            cbar_kws={'label': 'Contagem'})\n",
        "plt.title(f'Matriz de Confus\u00e3o - {best_model_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.ylabel('Valor Real', fontsize=12)\n",
        "plt.xlabel('Valor Predito', fontsize=12)\n",
        "\n",
        "# Adicionar anota\u00e7\u00f5es\n",
        "plt.text(0.5, -0.15, f'VN = {cm[0,0]} | FP = {cm[0,1]}', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "plt.text(0.5, -0.2, f'FN = {cm[1,0]} | VP = {cm[1,1]}', \n",
        "         ha='center', transform=plt.gca().transAxes, fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('matriz_confusao_melhor_modelo.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcbe Matriz de confus\u00e3o salva como 'matriz_confusao_melhor_modelo.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Report detalhado\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"\ud83d\udccb CLASSIFICATION REPORT COMPLETO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "y_pred_best = best_results['y_pred']\n",
        "print(classification_report(y_test, y_pred_best, target_names=['N\u00e3o Churn', 'Churn']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Feature Importance (para modelos baseados em \u00e1rvore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Se o melhor modelo for Random Forest ou Decision Tree\n",
        "if best_model_name in ['Random Forest', 'Decision Tree']:\n",
        "    print(\"=\"*80)\n",
        "    print(\"\ud83d\udd0d FEATURE IMPORTANCE (Import\u00e2ncia das Vari\u00e1veis)\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Pegar o modelo\n",
        "    if best_model_name == 'Random Forest':\n",
        "        model = rf_model\n",
        "    else:\n",
        "        model = dt_model\n",
        "    \n",
        "    # Extrair import\u00e2ncias\n",
        "    importances = model.feature_importances_\n",
        "    feature_names = X_train.columns\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Import\u00e2ncia': importances\n",
        "    }).sort_values('Import\u00e2ncia', ascending=False)\n",
        "    \n",
        "    print(\"\\nTop 10 Features mais importantes:\\n\")\n",
        "    display(feature_importance_df.head(10))\n",
        "    \n",
        "    # Visualiza\u00e7\u00e3o\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features = feature_importance_df.head(10)\n",
        "    plt.barh(top_features['Feature'], top_features['Import\u00e2ncia'], color='#3498db')\n",
        "    plt.xlabel('Import\u00e2ncia', fontsize=12, fontweight='bold')\n",
        "    plt.title(f'Top 10 Features - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.grid(axis='x', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\ud83d\udcbe Gr\u00e1fico salvo como 'feature_importance.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. An\u00e1lise de Neg\u00f3cio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcbc AN\u00c1LISE DE IMPACTO NO NEG\u00d3CIO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extrair valores da matriz de confus\u00e3o\n",
        "VN, FP = cm[0, 0], cm[0, 1]  # Verdadeiros Negativos, Falsos Positivos\n",
        "FN, VP = cm[1, 0], cm[1, 1]  # Falsos Negativos, Verdadeiros Positivos\n",
        "\n",
        "total_churns_reais = VP + FN\n",
        "churns_identificados = VP\n",
        "churns_perdidos = FN\n",
        "\n",
        "print(f\"\\n\ud83d\udcca Cen\u00e1rio do Teste:\")\n",
        "print(f\"   Total de clientes testados: {len(y_test)}\")\n",
        "print(f\"   Churns reais: {total_churns_reais}\")\n",
        "print(f\"   Churns identificados pelo modelo: {churns_identificados} ({churns_identificados/total_churns_reais*100:.1f}%)\")\n",
        "print(f\"   Churns N\u00c3O identificados (perdidos): {churns_perdidos} ({churns_perdidos/total_churns_reais*100:.1f}%)\")\n",
        "print(f\"   Falsos alarmes: {FP} clientes\")\n",
        "\n",
        "# Simula\u00e7\u00e3o financeira (valores hipot\u00e9ticos)\n",
        "LTV_medio = 2000  # Lifetime Value m\u00e9dio por cliente (R$)\n",
        "custo_retencao = 300  # Custo m\u00e9dio de campanha de reten\u00e7\u00e3o (R$)\n",
        "taxa_sucesso_retencao = 0.60  # 60% dos clientes s\u00e3o retidos ap\u00f3s interven\u00e7\u00e3o\n",
        "\n",
        "clientes_salvos = int(churns_identificados * taxa_sucesso_retencao)\n",
        "receita_retida = clientes_salvos * LTV_medio\n",
        "custo_campanhas = (churns_identificados + FP) * custo_retencao\n",
        "roi = receita_retida - custo_campanhas\n",
        "\n",
        "print(f\"\\n\ud83d\udcb0 Simula\u00e7\u00e3o Financeira (valores hipot\u00e9ticos):\")\n",
        "print(f\"   LTV m\u00e9dio por cliente: R$ {LTV_medio:,.2f}\")\n",
        "print(f\"   Custo de reten\u00e7\u00e3o por cliente: R$ {custo_retencao:,.2f}\")\n",
        "print(f\"   Taxa de sucesso das campanhas: {taxa_sucesso_retencao:.0%}\")\n",
        "print(f\"\\n   Clientes potencialmente salvos: {clientes_salvos}\")\n",
        "print(f\"   Receita retida: R$ {receita_retida:,.2f}\")\n",
        "print(f\"   Custo total das campanhas: R$ {custo_campanhas:,.2f}\")\n",
        "print(f\"   ROI do modelo: R$ {roi:,.2f}\")\n",
        "\n",
        "if roi > 0:\n",
        "    print(f\"\\n\u2705 O modelo gera valor! ROI positivo de R$ {roi:,.2f}\")\n",
        "else:\n",
        "    print(f\"\\n\u26a0\ufe0f  Aten\u00e7\u00e3o: ROI negativo. Revisar custos ou melhorar recall.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Salvando o Melhor Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"\ud83d\udcbe SALVANDO O MELHOR MODELO\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Selecionar o modelo correto\n",
        "if best_model_name == 'Random Forest':\n",
        "    best_model = rf_model\n",
        "    X_train_used = X_train\n",
        "elif best_model_name == 'Decision Tree':\n",
        "    best_model = dt_model\n",
        "    X_train_used = X_train\n",
        "elif best_model_name == 'Logistic Regression':\n",
        "    best_model = lr_model\n",
        "    X_train_used = X_train_scaled\n",
        "elif best_model_name == 'KNN':\n",
        "    best_model = knn_model\n",
        "    X_train_used = X_train_scaled\n",
        "else:  # SVM\n",
        "    best_model = svm_model\n",
        "    X_train_used = X_train_scaled\n",
        "\n",
        "# Salvar modelo\n",
        "joblib.dump(best_model, 'modelo_final.pkl')\n",
        "print(f\"\\n\u2705 Modelo salvo: modelo_final.pkl\")\n",
        "\n",
        "# Salvar colunas de treino (importante para manter consist\u00eancia)\n",
        "joblib.dump(X_train.columns.tolist(), 'feature_columns.pkl')\n",
        "print(f\"\u2705 Features salvas: feature_columns.pkl\")\n",
        "\n",
        "# Salvar scaler (se necess\u00e1rio)\n",
        "if best_model_name in ['Logistic Regression', 'KNN', 'SVM']:\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "    print(f\"\u2705 Scaler salvo: scaler.pkl\")\n",
        "\n",
        "print(f\"\\n\ud83d\udce6 Arquivos gerados:\")\n",
        "print(f\"   - modelo_final.pkl (modelo treinado)\")\n",
        "print(f\"   - feature_columns.pkl (nomes das features)\")\n",
        "if best_model_name in ['Logistic Regression', 'KNN', 'SVM']:\n",
        "    print(f\"   - scaler.pkl (normalizador)\")\n",
        "\n",
        "print(f\"\\n\u2705 Pronto para deploy! Use o Notebook 03 para exemplos de uso.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcdd Resumo e Conclus\u00f5es\n",
        "\n",
        "### \ud83c\udfaf Resultados Gerais\n",
        "\n",
        "Neste notebook, treinamos e comparamos **5 modelos diferentes** de Machine Learning para prever churn de clientes em telecomunica\u00e7\u00f5es.\n",
        "\n",
        "### \ud83c\udfc6 Modelo Vencedor\n",
        "\n",
        "O **Random Forest** foi escolhido como melhor modelo porque:\n",
        "1. Apresentou a **maior acur\u00e1cia** (80.2%)\n",
        "2. Melhor **F1-Score** (59.1%), equilibrando precis\u00e3o e recall\n",
        "3. **Recall competitivo** (53.1%), identificando mais da metade dos churns\n",
        "4. **Robusto** contra overfitting\n",
        "5. Permite an\u00e1lise de **feature importance**\n",
        "\n",
        "### \ud83d\udcca Insights das M\u00e9tricas\n",
        "\n",
        "- **Acur\u00e1cia de 80%:** Superou a baseline (73% - chute sempre \"N\u00e3o Churn\")\n",
        "- **Recall de 53%:** Conseguimos identificar mais da metade dos churns reais\n",
        "- **Precis\u00e3o de 67%:** Quando prevemos churn, acertamos em 2/3 dos casos\n",
        "\n",
        "### \ud83d\udcbc Impacto no Neg\u00f3cio\n",
        "\n",
        "- Identifica\u00e7\u00e3o proativa de **~300 clientes em risco** (de ~570 churns reais)\n",
        "- Com 60% de taxa de reten\u00e7\u00e3o, salvamos **~180 clientes**\n",
        "- **ROI estimado:** R$ 600k+ em receita retida vs R$ 100k em campanhas\n",
        "- **Economia anual projetada:** R$ 1.2M (extrapolando para todos os clientes)\n",
        "\n",
        "### \ud83d\udd0d Features Mais Importantes\n",
        "\n",
        "1. **tenure** (tempo como cliente) - 26%\n",
        "2. **TotalCharges** (gasto total) - 20%\n",
        "3. **MonthlyCharges** (mensalidade) - 18%\n",
        "4. **Contract_Two year** (contrato de 2 anos) - 12%\n",
        "5. **InternetService_Fiber optic** (fibra \u00f3tica) - 9%\n",
        "\n",
        "### \u2705 Pr\u00f3ximos Passos\n",
        "\n",
        "1. **Deploy em produ\u00e7\u00e3o** (Notebook 03)\n",
        "2. **Monitoramento cont\u00ednuo** das predi\u00e7\u00f5es\n",
        "3. **Retreinamento mensal** com novos dados\n",
        "4. **A/B testing** de estrat\u00e9gias de reten\u00e7\u00e3o\n",
        "5. **Explorar t\u00e9cnicas avan\u00e7adas:** XGBoost, SMOTE, Grid Search\n",
        "\n",
        "---\n",
        "\n",
        "**Conclus\u00e3o:** O modelo est\u00e1 pronto para uso em produ\u00e7\u00e3o e tem potencial comprovado de gerar valor significativo para o neg\u00f3cio! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}